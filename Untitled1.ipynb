{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-num_epochs NUM_EPOCHS] [-dataset DATASET] [-num_train NUM_TRAIN]\n",
      "                             [-num_val NUM_VAL] [-lr_schedule LR_SCHEDULE] [-only_plot ONLY_PLOT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\frafa\\AppData\\Roaming\\jupyter\\runtime\\kernel-7e691e1b-ce2b-4f47-93db-6969d27feaa7.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frafa\\anaconda3\\envs\\deepL_TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#misc\n",
    "import os\n",
    "import pickle as pkl\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "optim_dict = {\n",
    "\t\t'sgd': {\n",
    "\t\t\t'label': 'SGD',\n",
    "\t\t\t'lr': 1e-3\n",
    "\t\t},\n",
    "\t\t'sgd_momentum': {\n",
    "\t\t\t'label': 'SGD w/ momentum',\n",
    "\t\t\t'lr': 1e-3,\n",
    "\t\t\t'mu': 0.99\n",
    "\t\t},\n",
    "\t\t'sgd_nesterov': {\n",
    "\t\t\t'label': 'SGD w/ Nesterov momentum',\n",
    "\t\t\t'lr': 1e-3,\n",
    "\t\t\t'mu': 0.99,\n",
    "\t\t\t'nesterov': True\n",
    "\t\t},\n",
    "\t\t'sgd_weight_decay': {\n",
    "\t\t\t'label': 'SGDW',\n",
    "\t\t\t'lr': 1e-3,\n",
    "\t\t\t'mu': 0.99,\n",
    "\t\t\t'weight_decay': 1e-6\n",
    "\t\t},\n",
    "\t\t'sgd_lrd': {\n",
    "\t\t\t'label': 'SGD w/ momentum + LRD',\n",
    "\t\t\t'lr': 1e-3,\n",
    "\t\t\t'mu': 0.99,\n",
    "\t\t\t'lrd': 0.5\n",
    "\t\t},\n",
    "\t\t'adam': {\n",
    "\t\t\t'label': 'Adam',\n",
    "\t\t\t'lr': 1e-3\n",
    "\t\t},\n",
    "\t\t'adamW':{\n",
    "\t\t\t'label': 'AdamW',\n",
    "\t\t\t'lr': 1e-3,\n",
    "\t\t\t'weight_decay': 1e-4\n",
    "\t\t},\n",
    "\t\t'adam_l2':{\n",
    "\t\t\t'label': 'AdamL2',\n",
    "\t\t\t'lr': 1e-3,\n",
    "\t\t\t'l2_reg': 1e-4\n",
    "\t\t},\n",
    "\t\t'adam_lrd': {\n",
    "\t\t\t'label': 'Adam w/ LRD',\n",
    "\t\t\t'lr': 1e-3,\n",
    "\t\t\t'lrd': 0.5\n",
    "\t\t},\n",
    "\t\t'Radam': {\n",
    "\t\t\t'label': 'RAdam',\n",
    "\t\t\t'lr': 1e-3,\n",
    "\t\t\t'rectified': True\n",
    "\t\t},\n",
    "\t\t'RadamW': {\n",
    "\t\t\t'label': 'RAdamW',\n",
    "\t\t\t'lr': 1e-3,\n",
    "\t\t\t'rectified': True,\n",
    "\t\t\t'weight_decay': 1e-4\n",
    "\t\t},\n",
    "\t\t'Radam_lrd': {\n",
    "\t\t\t'label': 'RAdam w/ LRD',\n",
    "\t\t\t'lr': 1e-3,\n",
    "\t\t\t'rectified': True,\n",
    "\t\t\t'lrd': 0.5\n",
    "\t\t},\n",
    "\t\t'nadam': {\n",
    "\t\t\t'label': 'Nadam',\n",
    "\t\t\t'lr': 1e-3,\n",
    "\t\t\t'nesterov': True\n",
    "\t\t},\n",
    "\t\t'rmsprop': {\n",
    "\t\t\t'label': 'RMSprop',\n",
    "\t\t\t'lr': 1e-3,\n",
    "\t\t\t'beta2': 0.9,\n",
    "\t\t},\n",
    "\t\t'lookahead_sgd': {\n",
    "\t\t\t'label': 'Lookahead (SGD)',\n",
    "\t\t\t'lr': 1e-3,\n",
    "\t\t\t'mu': 0.99\n",
    "\t\t},\n",
    "\t\t'lookahead_adam': {\n",
    "\t\t\t'label': 'Lookahead (Adam)',\n",
    "\t\t\t'lr': 1e-3\n",
    "\t\t},\n",
    "\t\t'gradnoise_adam': {\n",
    "\t\t\t'label': 'Gradient Noise (Adam)',\n",
    "\t\t\t'lr': 1e-3\n",
    "\t\t},\n",
    "\t\t'graddropout_adam': {\n",
    "\t\t\t'label': 'Gradient Dropout (Adam)',\n",
    "\t\t\t'lr': 1e-3\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\n",
    "def split_optim_dict(d:dict) -> tuple:\n",
    "\t\"\"\"\n",
    "\tSplits an optimization dict into label and dict.\n",
    "\t\"\"\"\n",
    "\ttemp_d = deepcopy(d)\n",
    "\tlabel = temp_d['label']\n",
    "\tdel temp_d['label']\n",
    "\n",
    "\treturn label, temp_d\n",
    "\n",
    "\n",
    "def load_cifar(num_train=50000, num_val=2048):\n",
    "\t\"\"\"\n",
    "\tLoads a subset of the CIFAR dataset and returns it as a tuple.\n",
    "\t\"\"\"\n",
    "\ttransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "\n",
    "\ttrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\tval_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "\ttrain_dataset, _ = torch.utils.data.random_split(train_dataset, lengths=[num_train, len(train_dataset)-num_train])\n",
    "\tval_dataset, _ = torch.utils.data.random_split(val_dataset, lengths=[num_val, len(val_dataset)-num_val])\n",
    "\n",
    "\treturn train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def load_mnist(filename='data/mnist.npz', num_train=4096, num_val=512):\n",
    "\t\"\"\"\n",
    "\tLoads a subset of the grayscale MNIST dataset and returns it as a tuple.\n",
    "\t\"\"\"\n",
    "\tdata = np.load(filename)\n",
    "\n",
    "\tx_train = data['X_train'][:num_train].astype('float32')\n",
    "\ty_train = data['y_train'][:num_train].astype('int32')\n",
    "\n",
    "\tx_valid = data['X_valid'][:num_val].astype('float32')\n",
    "\ty_valid = data['y_valid'][:num_val].astype('int32')\n",
    "\n",
    "\ttrain_dataset = Dataset(x_train, y_train)\n",
    "\tval_dataset = Dataset(x_valid, y_valid)\n",
    "\n",
    "\treturn train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def task_to_optimizer(task:str) -> torch.optim.Optimizer:\n",
    "\t\"\"\"\n",
    "\tTakes a task as string and returns its respective optimizer class.\n",
    "\t\"\"\"\n",
    "\toptimizer = None\n",
    "\n",
    "\tif 'sgd' in task.lower():\n",
    "\t\toptimizer = getattr(optimizers, 'SGD')\n",
    "\tif 'adam' in task.lower():\n",
    "\t\toptimizer = getattr(optimizers, 'Adam')\n",
    "\tif 'rmsprop' in task.lower():\n",
    "\t\toptimizer = getattr(optimizers, 'RMSProp')\n",
    "\t\n",
    "\tif optimizer is None:\n",
    "\t\traise ValueError(f'Optimizer for task \\'{task}\\' was not recognized!')\n",
    "\n",
    "\treturn optimizer\n",
    "\n",
    "\n",
    "def wrap_optimizer(task:str, optimizer):\n",
    "\t\"\"\"\n",
    "\tWraps an instantiated optimizer according to its task specified as a string.\n",
    "\t\"\"\"\n",
    "\tif 'gradnoise' in task.lower():\n",
    "\t\toptimizer = optimizers.GradientNoise(optimizer, eta=0.3, gamma=0.55)\n",
    "\n",
    "\tif 'graddropout' in task.lower():\n",
    "\t\toptimizer = optimizers.GradientDropout(optimizer, grad_retain=0.9)\n",
    "\n",
    "\tif 'lookahead' in task.lower():\n",
    "\t\toptimizer = optimizers.Lookahead(optimizer, k=5, alpha=0.5)\n",
    "\n",
    "\treturn optimizer\n",
    "\n",
    "\n",
    "class AvgLoss():\n",
    "\t\"\"\"\n",
    "\tUtility class that tracks the average loss.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.sum, self.avg, self.n = 0, 0, 0\n",
    "\t\tself.losses = []\n",
    "\n",
    "\tdef __iadd__(self, other):\n",
    "\t\ttry:\n",
    "\t\t\tloss = other.data.numpy()\n",
    "\t\texcept:\n",
    "\t\t\tloss = other\n",
    "\t\t\n",
    "\t\tif isinstance(other, list):\n",
    "\t\t\tself.losses.extend(other)\n",
    "\t\t\tself.sum += np.sum(other)\n",
    "\t\t\tself.n += len(other)\n",
    "\t\telse:\n",
    "\t\t\tself.losses.append(float(loss))\n",
    "\t\t\tself.sum += loss\n",
    "\t\t\tself.n += 1\n",
    "\n",
    "\t\tself.avg = self.sum / self.n\n",
    "\n",
    "\t\treturn self\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\treturn '{0:.4f}'.format(round(self.avg, 4))\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.losses)\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "\tdef __init__(self, X, y):\n",
    "\t\tself.X = X\n",
    "\t\tself.y = y\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.X)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def save_losses(losses, dataset:str, filename:str):\n",
    "\tif not os.path.exists(f'losses_{dataset}/'): os.makedirs(f'losses_{dataset}/')\n",
    "\twith open(f'losses_{dataset}/{filename}.pkl', 'wb') as f:\n",
    "\t\tpkl.dump(losses, f, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_losses(dataset:str, filename:str):\n",
    "\ttry:\n",
    "\t\twith open(f'losses_{dataset}/{filename}.pkl', 'rb') as f:\n",
    "\t\t\treturn pkl.load(f)\n",
    "\texcept:\n",
    "\t\treturn None\n",
    "\n",
    "\n",
    "def plot_mnist(X):\n",
    "\tidx, dim, classes = 0, 28, 10\n",
    "\tcanvas = np.zeros((dim*classes, classes*dim))\n",
    "\n",
    "\tfor i in range(classes):\n",
    "\t\tfor j in range(classes):\n",
    "\t\t\tcanvas[i*dim:(i+1)*dim, j*dim:(j+1)*dim] = X[idx].reshape((dim, dim))\n",
    "\t\t\tidx += 1\n",
    "\n",
    "\tsns.set(style='darkgrid')\n",
    "\tplt.figure(figsize=(9, 9))\n",
    "\tplt.axis('off')\n",
    "\tplt.tight_layout(pad=0)\n",
    "\tplt.imshow(canvas, cmap='gray')\n",
    "\tplt.savefig('mnist_examples.png')\n",
    "\tplt.clf()\n",
    "\n",
    "\n",
    "def plot_loss(losses, val_losses, num_epochs):\n",
    "\tsns.set(style='darkgrid')\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\tplt.plot(np.linspace(0, num_epochs, num=len(losses)), losses.losses, label='Training loss')\n",
    "\tplt.plot(np.linspace(0, num_epochs, num=len(val_losses)), val_losses.losses, label='Validation loss')\n",
    "\tplt.tight_layout(pad=2)\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel('Negative log likelihood')\n",
    "\tplt.savefig('loss.png')\n",
    "\tplt.clf()\n",
    "\n",
    "\n",
    "def plot_losses(losses, val_losses, labels, num_epochs, title, plot_val=False, yscale_log=False, max_epochs=None):\n",
    "\tsns.set(style='darkgrid')\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\tcolors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:cyan', 'tab:olive']\n",
    "\n",
    "\tfor i in range(len(losses)):\n",
    "\t\tplt.plot(np.linspace(0, num_epochs, num=len(losses[i])), smooth(losses[i].losses, 81), label=labels[i], alpha=1, c=colors[i])\n",
    "\t\tplt.plot(np.linspace(0, num_epochs, num=len(losses[i])), smooth(losses[i].losses, 21), alpha=0.25, c=colors[i])\n",
    "\t\tif plot_val:\n",
    "\t\t\tplt.plot(np.linspace(0, num_epochs, num=len(val_losses[i])), smooth(val_losses[i].losses, 81), alpha=1, linestyle='--', c=colors[i])\n",
    "\n",
    "\tplt.tight_layout(pad=2)\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel('Cross-entropy')\n",
    "\tif yscale_log:\n",
    "\t\tplt.yscale('log')\n",
    "\tif max_epochs is not None:\n",
    "\t\tplt.xlim(-1, max_epochs)\n",
    "\tplt.ylim(0, 3)\n",
    "\tplt.title('CNN benchmark on CIFAR-10' if title == 'cifar' else 'MLP benchmark on MNIST')\n",
    "\tplt.legend(loc='upper right')\n",
    "\tplt.savefig(f'loss_{title}.png')\n",
    "\tplt.clf()\n",
    "\n",
    "\n",
    "def smooth(signal, kernel_size, polyorder=3):\n",
    "\treturn savgol_filter(signal, kernel_size, polyorder)\n",
    "\n",
    "##################################################\n",
    "#networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch import utils\n",
    "from torch.nn import Parameter\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\t\"\"\"\n",
    "\tA small multilayer perceptron with parameters that we can optimize for the task.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, num_features=784, num_hidden=64, num_outputs=10):\n",
    "\t\tsuper(MLP, self).__init__()\n",
    "\n",
    "\t\tself.W_1 = Parameter(init.xavier_normal_(torch.Tensor(num_hidden, num_features)))\n",
    "\t\tself.b_1 = Parameter(init.constant_(torch.Tensor(num_hidden), 0))\n",
    "\n",
    "\t\tself.W_2 = Parameter(init.xavier_normal_(torch.Tensor(num_outputs, num_hidden)))\n",
    "\t\tself.b_2 = Parameter(init.constant_(torch.Tensor(num_outputs), 0))\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = F.relu(F.linear(x, self.W_1, self.b_1))\n",
    "\t\tx = F.linear(x, self.W_2, self.b_2)\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\t\"\"\"\n",
    "\tA small convolutional neural network with parameters that we can optimize for the task.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, num_layers=4, num_filters=64, num_classes=10, input_size=(3, 32, 32)):\n",
    "\t\tsuper(CNN, self).__init__()\n",
    "\n",
    "\t\tself.channels = input_size[0]\n",
    "\t\tself.height = input_size[1]\n",
    "\t\tself.width = input_size[2]\n",
    "\t\tself.num_filters = num_filters\n",
    "\n",
    "\t\tself.conv_in = nn.Conv2d(self.channels, self.num_filters, kernel_size=5, padding=2)\n",
    "\t\tcnn = []\n",
    "\t\tfor _ in range(num_layers):\n",
    "\t\t\tcnn.append(nn.Conv2d(self.num_filters, self.num_filters, kernel_size=3, padding=1))\n",
    "\t\t\tcnn.append(nn.BatchNorm2d(self.num_filters))\n",
    "\t\t\tcnn.append(nn.ReLU())\n",
    "\t\tself.cnn = nn.Sequential(*cnn)\n",
    "\n",
    "\t\tself.out_lin = nn.Linear(self.num_filters*self.width*self.height, num_classes)\n",
    "\n",
    "\t\tif torch.cuda.is_available():\n",
    "\t\t\tself.cuda()\n",
    "\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tif torch.cuda.is_available():\n",
    "\t\t\tx = x.cuda()\n",
    "\n",
    "\t\tx = F.relu(self.conv_in(x))\n",
    "\t\tx = self.cnn(x)\n",
    "\t\tx = x.reshape(x.size(0), -1)\n",
    "\n",
    "\t\treturn self.out_lin(x)\n",
    "\n",
    "\n",
    "def fit(net, data, optimizer, batch_size=128, num_epochs=250, lr_schedule=False):\n",
    "\t\"\"\"\n",
    "\tFits parameters of a network `net` using `data` as training data and a given `optimizer`.\n",
    "\t\"\"\"\n",
    "\ttrain_generator = utils.data.DataLoader(data[0], batch_size=batch_size)\n",
    "\tval_generator = utils.data.DataLoader(data[1], batch_size=batch_size)\n",
    "\n",
    "\tlosses = AvgLoss()\n",
    "\tval_losses = AvgLoss()\n",
    "\n",
    "\tif lr_schedule:\n",
    "\t\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "\tfor epoch in range(num_epochs+1):\n",
    "\n",
    "\t\tepoch_loss = AvgLoss()\n",
    "\t\tepoch_val_loss = AvgLoss()\n",
    "\n",
    "\t\tfor x, y in val_generator:\n",
    "\t\t\ty = y.type(torch.LongTensor)\n",
    "\t\t\tif torch.cuda.is_available(): y = y.cuda()\n",
    "\t\t\tepoch_val_loss += F.cross_entropy(net(x), y).cpu()\n",
    "\n",
    "\t\tfor x, y in train_generator:\n",
    "\t\t\ty = y.type(torch.LongTensor)\n",
    "\t\t\tif torch.cuda.is_available(): y = y.cuda()\n",
    "\t\t\tloss = F.cross_entropy(net(x), y).cpu()\n",
    "\t\t\tepoch_loss += loss\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\tif lr_schedule:\n",
    "\t\t\tscheduler.step(epoch_loss.avg)\n",
    "\n",
    "\t\tif epoch % 2 == 0:\n",
    "\t\t\tprint(f'Epoch {epoch}/{num_epochs}, loss: {epoch_loss}, val loss: {epoch_val_loss}')\n",
    "\n",
    "\t\tlosses += epoch_loss.losses\n",
    "\t\tval_losses += epoch_val_loss.losses\n",
    "\n",
    "\treturn losses, val_losses\n",
    "\n",
    "##################################################\n",
    "#optimizers\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import Optimizer\n",
    "from torch.distributions import Bernoulli, Normal\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent. Also includes implementations of momentum,\n",
    "    Nesterov's momentum, L2 regularization, SGDW and Learning Rate Dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr, mu=0, nesterov=False, weight_decay=0, lrd=1):\n",
    "        defaults = {'lr': lr, 'mu': mu, 'nesterov': nesterov, 'weight_decay': weight_decay, 'lrd': lrd}\n",
    "        super(SGD, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            lr = group['lr']\n",
    "            mu = group['mu']\n",
    "            nesterov = group['nesterov']\n",
    "            weight_decay = group['weight_decay']\n",
    "            lrd_bernoulli = Bernoulli(probs=group['lrd'])\n",
    "\n",
    "            if mu != 0 and 'v' not in group:\n",
    "                group['v'] = []\n",
    "                if nesterov:\n",
    "                    group['theta'] = []\n",
    "                for param in group['params']:\n",
    "                    group['v'].append(torch.zeros_like(param))\n",
    "                    if nesterov:\n",
    "                        theta_param = torch.ones_like(param).mul_(param.data)\n",
    "                        group['theta'].append(theta_param)\n",
    "\n",
    "            for idx, param in enumerate(group['params']):\n",
    "                param.grad.data -= weight_decay * param.data\n",
    "                lrd_mask = lrd_bernoulli.sample(param.size()).to(param.device)\n",
    "\n",
    "                if mu != 0:\n",
    "                    v = group['v'][idx]\n",
    "                    v = mu * v - lr * param.grad.data\n",
    "                    group['v'][idx] = v\n",
    "\n",
    "                    if nesterov:\n",
    "                        group['theta'][idx] += lrd_mask * v\n",
    "                        param.data = group['theta'][idx] + mu * v\n",
    "\n",
    "                    else:\n",
    "                        param.data += lrd_mask * v\n",
    "\n",
    "                else:\n",
    "                    param.data -= lrd_mask * lr * param.grad.data\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"\n",
    "    Adam as proposed by https://arxiv.org/abs/1412.6980.\n",
    "    Also includes a number of proposed extensions to the the Adam algorithm,\n",
    "    such as Nadam, L2 regularization, AdamW, RAdam and Learning Rate Dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr, beta1=0.9, beta2=0.999, nesterov=False, l2_reg=0, weight_decay=0, rectified=False, lrd=1, eps=1e-8):\n",
    "        defaults = {'lr': lr, 'beta1': beta1, 'beta2': beta2, 'nesterov': nesterov, 'l2_reg': l2_reg,\n",
    "                    'weight_decay': weight_decay, 'rectified': rectified, 'lrd': lrd, 'eps': eps}\n",
    "        super(Adam, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            lr = group['lr']\n",
    "            beta1 = group['beta1']\n",
    "            beta2 = group['beta2']\n",
    "            nesterov = group['nesterov']\n",
    "            l2_reg = group['l2_reg']\n",
    "            weight_decay = group['weight_decay']\n",
    "            rectified = group['rectified']\n",
    "            lrd_bernoulli = Bernoulli(probs=group['lrd'])\n",
    "            eps = group['eps']\n",
    "\n",
    "            if 'm' not in group and 'v' not in group:\n",
    "                group['m'] = []\n",
    "                group['v'] = []\n",
    "                group['t'] = 1\n",
    "                if nesterov:\n",
    "                    group['prev_grad'] = []\n",
    "                for param in group['params']:\n",
    "                    group['m'].append(torch.zeros_like(param))\n",
    "                    group['v'].append(torch.zeros_like(param))\n",
    "                    if nesterov:\n",
    "                        group['prev_grad'].append(torch.zeros_like(param))\n",
    "\n",
    "            for idx, param in enumerate(group['params']):\n",
    "                if l2_reg:\n",
    "                    param.grad.data += l2_reg * param.data\n",
    "\n",
    "                if nesterov:\n",
    "                    grad = group['prev_grad'][idx]\n",
    "                else:\n",
    "                    grad = param.grad.data\n",
    "\n",
    "                lrd_mask = lrd_bernoulli.sample(param.size()).to(param.device)\n",
    "\n",
    "                m = group['m'][idx]\n",
    "                v = group['v'][idx]\n",
    "                t = group['t']\n",
    "                m = beta1 * m + (1 - beta1) * grad\n",
    "                v = beta2 * v + (1 - beta2) * grad**2\n",
    "                m_hat = m / (1 - beta1**t)\n",
    "                v_hat = v / (1 - beta2**t)\n",
    "\n",
    "                if nesterov:\n",
    "                    group['prev_grad'][idx] = param.grad.data\n",
    "\n",
    "                if rectified:\n",
    "                    rho_inf = 2 / (1 - beta2) - 1\n",
    "                    rho = rho_inf - 2 * t * beta2**t / (1 - beta2**t)\n",
    "                    if rho >= 5:\n",
    "                        numerator = (1 - beta2**t) * (rho - 4) * (rho - 2) * rho_inf\n",
    "                        denominator = (rho_inf - 4) * (rho_inf - 2) * rho\n",
    "                        r = np.sqrt(numerator / denominator)\n",
    "                        param.data += - lrd_mask * lr * r * m_hat / (torch.sqrt(v) + eps)\n",
    "                    else:\n",
    "                        param.data += - lrd_mask * lr * m_hat\n",
    "                else:\n",
    "                    param.data += - lrd_mask * lr * m_hat / (torch.sqrt(v_hat) + eps)\n",
    "\n",
    "                if weight_decay:\n",
    "                    param.data -= weight_decay * param.data\n",
    "\n",
    "                group['m'][idx] = m\n",
    "                group['v'][idx] = v\n",
    "\n",
    "            group['t'] += 1\n",
    "\n",
    "\n",
    "class RMSProp(Adam):\n",
    "    \"\"\"\n",
    "    RMSprop as proposed by http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf.\n",
    "    Note that this implementation, unlike the original RMSprop, uses bias-corrected moments.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr, beta2):\n",
    "        super(RMSProp, self).__init__(params, lr, beta2=beta2, beta1=0)\n",
    "\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    \"\"\"\n",
    "    Lookahead Optimization as proposed by https://arxiv.org/abs/1907.08610.\n",
    "    This is a wrapper class that can be applied to an instantiated optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = optimizer.param_groups\n",
    "\n",
    "        self.counter = 0\n",
    "        for group in optimizer.param_groups:\n",
    "            group['phi'] = []\n",
    "            for param in group['params']:\n",
    "                phi_param = torch.ones_like(param).mul_(param.data)\n",
    "                group['phi'].append(phi_param)\n",
    "\n",
    "    def step(self):\n",
    "        if self.counter == self.k:\n",
    "            for group_idx, group in enumerate(self.param_groups):\n",
    "                for idx, _ in enumerate(group['phi']):\n",
    "                    theta = self.optimizer.param_groups[group_idx]['params'][idx].data\n",
    "                    group['phi'][idx] = group['phi'][idx] + self.alpha * (theta - group['phi'][idx])\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "class GradientNoise(Optimizer):\n",
    "    \"\"\"\n",
    "    Gradient Noise as proposed by https://arxiv.org/abs/1511.06807.\n",
    "    This is a wrapper class that can be applied to an instantiated optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, eta=0.3, gamma=0.55):\n",
    "        self.optimizer = optimizer\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.t = 0\n",
    "        self.param_groups = optimizer.param_groups\n",
    "\n",
    "    def step(self):\n",
    "        normal = torch.empty(1).normal_(mean=0, std=np.sqrt(self.eta/((1+self.t)**self.gamma)))\\\n",
    "            .to(self.optimizer.param_groups[0]['params'][0].device)\n",
    "        for group_idx, group in enumerate(self.param_groups):\n",
    "            for idx, param in enumerate(group['params']):\n",
    "                self.optimizer.param_groups[group_idx]['params'][idx].grad.data += normal\n",
    "                self.optimizer.step()\n",
    "                self.t += 1\n",
    "\n",
    "\n",
    "class GradientDropout(Optimizer):\n",
    "    \"\"\"\n",
    "    Gradient dropout as proposed by https://arxiv.org/abs/1912.00144.\n",
    "    This is a wrapper class that can be applied to an instantiated optimizer.\n",
    "    Note that this method does not improve optimization significantly and\n",
    "    is only here for comparison to Learning Rate Dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, grad_retain=0.9):\n",
    "        self.optimizer = optimizer\n",
    "        self.grad_retain = grad_retain\n",
    "        self.grad_bernoulli = Bernoulli(probs=grad_retain)\n",
    "        self.param_groups = optimizer.param_groups\n",
    "\n",
    "    def step(self):\n",
    "        for group_idx, group in enumerate(self.param_groups):\n",
    "            for idx, param in enumerate(group['params']):\n",
    "                grad_mask = self.grad_bernoulli.sample(param.size()).to(param.device)\n",
    "                self.optimizer.param_groups[group_idx]['params'][idx].grad.data *= grad_mask\n",
    "                self.optimizer.step()\n",
    "    \n",
    "\n",
    "##################################################\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\tparser.add_argument('-num_epochs', type=int, default=30)\n",
    "\tparser.add_argument('-dataset', type=str, default='cifar')\n",
    "\tparser.add_argument('-num_train', type=int, default=50000)\n",
    "\tparser.add_argument('-num_val', type=int, default=2048)\n",
    "\tparser.add_argument('-lr_schedule', type=bool, default=True)\n",
    "\tparser.add_argument('-only_plot', type=bool, default=True)\n",
    "\targs = parser.parse_args()\n",
    "\n",
    "\tdata = getattr(misc, 'load_'+args.dataset)(\n",
    "\t\tnum_train=args.num_train,\n",
    "\t\tnum_val=args.num_val\n",
    "\t)\n",
    "\n",
    "\tprint(f'Loaded data partitions: ({len(data[0])}), ({len(data[1])})')\n",
    "\t\n",
    "\topt_tasks = [\n",
    "\t\t'sgd',\n",
    "\t\t'sgd_momentum',\n",
    "\t\t'sgd_nesterov',\n",
    "\t\t'sgd_weight_decay',\n",
    "\t\t'sgd_lrd',\n",
    "\t\t'rmsprop',\n",
    "\t\t'adam',\n",
    "\t\t'adam_l2',\n",
    "\t\t'adamW',\n",
    "\t\t'adam_lrd',\n",
    "\t\t'Radam',\n",
    "\t\t'RadamW',\n",
    "\t\t'Radam_lrd',\n",
    "\t\t'nadam',\n",
    "\t\t'lookahead_sgd',\n",
    "\t\t'lookahead_adam',\n",
    "\t\t'gradnoise_adam',\n",
    "\t\t'graddropout_adam'\n",
    "\t]\n",
    "\topt_losses, opt_val_losses, opt_labels = [], [], []\n",
    "\n",
    "\tdef do_stuff(opt):\n",
    "\t\tprint(f'\\nTraining {opt} for {args.num_epochs} epochs...')\n",
    "\t\tnet = CNN() if args.dataset == 'cifar' else MLP()\n",
    "\t\t_, kwargs = split_optim_dict(optim_dict[opt])\n",
    "\t\toptimizer = task_to_optimizer(opt)(\n",
    "\t\t\tparams=net.parameters(),\n",
    "\t\t\t**kwargs\n",
    "\t\t)\n",
    "\t\toptimizer = wrap_optimizer(opt, optimizer)\n",
    "\n",
    "\t\treturn fit(net, data, optimizer, num_epochs=args.num_epochs, lr_schedule=True)\n",
    "\n",
    "\tfor opt in opt_tasks:\n",
    "\t\tif args.only_plot:\n",
    "\t\t\tprint(\"1\")\n",
    "\t\t\tlosses = load_losses(dataset=args.dataset, filename=opt)\n",
    "\t\t\tval_losses = load_losses(dataset=args.dataset, filename=opt+'_val')\n",
    "\t\telse:\n",
    "\t\t\tprint(\"2\")\n",
    "\t\t\tlosses, val_losses = do_stuff(opt)\n",
    "\t\t\tsave_losses(losses, dataset=args.dataset, filename=opt)\n",
    "\t\t\tsave_losses(val_losses, dataset=args.dataset, filename=opt+'_val')\n",
    "\n",
    "\t\tif losses is not None:\n",
    "\t\t\topt_losses.append(losses)\n",
    "\t\t\topt_val_losses.append(val_losses)\n",
    "\t\t\topt_labels.append(split_optim_dict(optim_dict[opt])[0])\n",
    "\n",
    "\tif torch.cuda.is_available():\n",
    "\t\tprint(3)\n",
    "\t\tassert len(opt_losses) == len(opt_val_losses)\n",
    "\t\tplot_losses(\n",
    "\t\t\tlosses=opt_losses,\n",
    "\t\t\tval_losses=opt_val_losses,\n",
    "\t\t\tlabels=opt_labels,\n",
    "\t\t\tnum_epochs=args.num_epochs,\n",
    "\t\t\ttitle=args.dataset,\n",
    "\t\t\tplot_val=False,\n",
    "\t\t\tyscale_log=False,\n",
    "\t\t\tmax_epochs=30\n",
    "\t\t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
